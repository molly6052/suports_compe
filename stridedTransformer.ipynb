{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## StridedTransformer-Pose3Dをダウンロード後に下記のコードで姿勢推定を実行する\n",
        "# !git clone https://github.com/Vegetebird/StridedTransformer-Pose3D"
      ],
      "metadata": {
        "id": "88ER5a_mR2KT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwT_SRDSeFkh",
        "outputId": "d30f0af9-2a01-4f10-e8ec-821416386398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from yacs) (6.0)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 KB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from filterpy) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from filterpy) (1.7.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from filterpy) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->filterpy) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->filterpy) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->filterpy) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->filterpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy) (1.15.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=8b18a53999f6f8db3a23cc02f6c28ce4d3a1bbba1e790806dc28922e70740cba\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/f6/cb/40331472edf4fd399b8cad02973c6acbdf26898342928327fe\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==2.2.2\n",
            "  Downloading matplotlib-2.2.2.tar.gz (37.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==2.2.2) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==2.2.2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==2.2.2) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from matplotlib==2.2.2) (2022.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==2.2.2) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==2.2.2) (1.4.4)\n",
            "Building wheels for collected packages: matplotlib\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-2.2.2-cp38-cp38-linux_x86_64.whl size=10551390 sha256=146cf028410d944435c37e65588a53e6d3425624ed218f8ceebd965d5819d25e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/99/36/584361dc7788ea9af2bf4dc89881e8c4cc251c15c8dd16dcf6\n",
            "Successfully built matplotlib\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.8.0 requires matplotlib>=3.1.1, but you have matplotlib 2.2.2 which is incompatible.\n",
            "mizani 0.7.3 requires matplotlib>=3.1.1, but you have matplotlib 2.2.2 which is incompatible.\n",
            "datascience 0.17.5 requires matplotlib>=3.0.0, but you have matplotlib 2.2.2 which is incompatible.\n",
            "arviz 0.12.1 requires matplotlib>=3.0, but you have matplotlib 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-2.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install yacs\n",
        "!pip install filterpy\n",
        "!pip install einops\n",
        "!pip install matplotlib==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhz4isEdbWN3",
        "outputId": "4d547db6-7784-4234-f925-955dbabf74fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j2ATkxrmsSD"
      },
      "outputs": [],
      "source": [
        "!cp -r '/content/drive/MyDrive/Colab Notebooks/StridedTransformer-Pose3D' '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrOulGy1dWeE",
        "outputId": "75e4cc3f-a7fe-497c-b57a-4d66a9361854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/StridedTransformer-Pose3D\n"
          ]
        }
      ],
      "source": [
        "%cd /content/StridedTransformer-Pose3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fgQSRP5YW_7"
      },
      "outputs": [],
      "source": [
        "!cp '/content/drive/MyDrive/Colab Notebooks/2018WCB_60_P2_SELLO Edwin_BOT_NAZIR BIN Abdou_MAD.mp4' '/content/StridedTransformer-Pose3D/demo/video'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z8tZKjRdYms",
        "outputId": "bb9690da-ec7a-4d96-a7ef-59d4b5dca42d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/StridedTransformer-Pose3D/demo/lib/hrnet/lib/models/pose_hrnet.py:487: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  or self.pretrained_layers[0] is '*':\n",
            "\n",
            "Generating 2D pose...\n",
            "100% 4909/4909 [10:23<00:00,  7.87it/s]\n",
            "Generating 2D pose successful!\n",
            "model_dict keys:  odict_keys(['encoder.0.weight', 'encoder.0.bias', 'encoder.1.weight', 'encoder.1.bias', 'encoder.1.running_mean', 'encoder.1.running_var', 'encoder.1.num_batches_tracked', 'Transformer.pos_embedding', 'Transformer.model.layers.0.self_attn.linears.0.weight', 'Transformer.model.layers.0.self_attn.linears.0.bias', 'Transformer.model.layers.0.self_attn.linears.1.weight', 'Transformer.model.layers.0.self_attn.linears.1.bias', 'Transformer.model.layers.0.self_attn.linears.2.weight', 'Transformer.model.layers.0.self_attn.linears.2.bias', 'Transformer.model.layers.0.self_attn.linears.3.weight', 'Transformer.model.layers.0.self_attn.linears.3.bias', 'Transformer.model.layers.0.feed_forward.w_1.weight', 'Transformer.model.layers.0.feed_forward.w_1.bias', 'Transformer.model.layers.0.feed_forward.w_2.weight', 'Transformer.model.layers.0.feed_forward.w_2.bias', 'Transformer.model.layers.0.sublayer.0.norm.a_2', 'Transformer.model.layers.0.sublayer.0.norm.b_2', 'Transformer.model.layers.0.sublayer.1.norm.a_2', 'Transformer.model.layers.0.sublayer.1.norm.b_2', 'Transformer.model.layers.1.self_attn.linears.0.weight', 'Transformer.model.layers.1.self_attn.linears.0.bias', 'Transformer.model.layers.1.self_attn.linears.1.weight', 'Transformer.model.layers.1.self_attn.linears.1.bias', 'Transformer.model.layers.1.self_attn.linears.2.weight', 'Transformer.model.layers.1.self_attn.linears.2.bias', 'Transformer.model.layers.1.self_attn.linears.3.weight', 'Transformer.model.layers.1.self_attn.linears.3.bias', 'Transformer.model.layers.1.feed_forward.w_1.weight', 'Transformer.model.layers.1.feed_forward.w_1.bias', 'Transformer.model.layers.1.feed_forward.w_2.weight', 'Transformer.model.layers.1.feed_forward.w_2.bias', 'Transformer.model.layers.1.sublayer.0.norm.a_2', 'Transformer.model.layers.1.sublayer.0.norm.b_2', 'Transformer.model.layers.1.sublayer.1.norm.a_2', 'Transformer.model.layers.1.sublayer.1.norm.b_2', 'Transformer.model.layers.2.self_attn.linears.0.weight', 'Transformer.model.layers.2.self_attn.linears.0.bias', 'Transformer.model.layers.2.self_attn.linears.1.weight', 'Transformer.model.layers.2.self_attn.linears.1.bias', 'Transformer.model.layers.2.self_attn.linears.2.weight', 'Transformer.model.layers.2.self_attn.linears.2.bias', 'Transformer.model.layers.2.self_attn.linears.3.weight', 'Transformer.model.layers.2.self_attn.linears.3.bias', 'Transformer.model.layers.2.feed_forward.w_1.weight', 'Transformer.model.layers.2.feed_forward.w_1.bias', 'Transformer.model.layers.2.feed_forward.w_2.weight', 'Transformer.model.layers.2.feed_forward.w_2.bias', 'Transformer.model.layers.2.sublayer.0.norm.a_2', 'Transformer.model.layers.2.sublayer.0.norm.b_2', 'Transformer.model.layers.2.sublayer.1.norm.a_2', 'Transformer.model.layers.2.sublayer.1.norm.b_2', 'Transformer.model.norm.a_2', 'Transformer.model.norm.b_2', 'Transformer_reduce.model.pos_embedding_1', 'Transformer_reduce.model.pos_embedding_2', 'Transformer_reduce.model.pos_embedding_3', 'Transformer_reduce.model.layers.0.self_attn.linears.0.weight', 'Transformer_reduce.model.layers.0.self_attn.linears.0.bias', 'Transformer_reduce.model.layers.0.self_attn.linears.1.weight', 'Transformer_reduce.model.layers.0.self_attn.linears.1.bias', 'Transformer_reduce.model.layers.0.self_attn.linears.2.weight', 'Transformer_reduce.model.layers.0.self_attn.linears.2.bias', 'Transformer_reduce.model.layers.0.self_attn.linears.3.weight', 'Transformer_reduce.model.layers.0.self_attn.linears.3.bias', 'Transformer_reduce.model.layers.0.feed_forward.w_1.weight', 'Transformer_reduce.model.layers.0.feed_forward.w_1.bias', 'Transformer_reduce.model.layers.0.feed_forward.w_2.weight', 'Transformer_reduce.model.layers.0.feed_forward.w_2.bias', 'Transformer_reduce.model.layers.0.sublayer.0.norm.a_2', 'Transformer_reduce.model.layers.0.sublayer.0.norm.b_2', 'Transformer_reduce.model.layers.0.sublayer.1.norm.a_2', 'Transformer_reduce.model.layers.0.sublayer.1.norm.b_2', 'Transformer_reduce.model.layers.1.self_attn.linears.0.weight', 'Transformer_reduce.model.layers.1.self_attn.linears.0.bias', 'Transformer_reduce.model.layers.1.self_attn.linears.1.weight', 'Transformer_reduce.model.layers.1.self_attn.linears.1.bias', 'Transformer_reduce.model.layers.1.self_attn.linears.2.weight', 'Transformer_reduce.model.layers.1.self_attn.linears.2.bias', 'Transformer_reduce.model.layers.1.self_attn.linears.3.weight', 'Transformer_reduce.model.layers.1.self_attn.linears.3.bias', 'Transformer_reduce.model.layers.1.feed_forward.w_1.weight', 'Transformer_reduce.model.layers.1.feed_forward.w_1.bias', 'Transformer_reduce.model.layers.1.feed_forward.w_2.weight', 'Transformer_reduce.model.layers.1.feed_forward.w_2.bias', 'Transformer_reduce.model.layers.1.sublayer.0.norm.a_2', 'Transformer_reduce.model.layers.1.sublayer.0.norm.b_2', 'Transformer_reduce.model.layers.1.sublayer.1.norm.a_2', 'Transformer_reduce.model.layers.1.sublayer.1.norm.b_2', 'Transformer_reduce.model.layers.2.self_attn.linears.0.weight', 'Transformer_reduce.model.layers.2.self_attn.linears.0.bias', 'Transformer_reduce.model.layers.2.self_attn.linears.1.weight', 'Transformer_reduce.model.layers.2.self_attn.linears.1.bias', 'Transformer_reduce.model.layers.2.self_attn.linears.2.weight', 'Transformer_reduce.model.layers.2.self_attn.linears.2.bias', 'Transformer_reduce.model.layers.2.self_attn.linears.3.weight', 'Transformer_reduce.model.layers.2.self_attn.linears.3.bias', 'Transformer_reduce.model.layers.2.feed_forward.w_1.weight', 'Transformer_reduce.model.layers.2.feed_forward.w_1.bias', 'Transformer_reduce.model.layers.2.feed_forward.w_2.weight', 'Transformer_reduce.model.layers.2.feed_forward.w_2.bias', 'Transformer_reduce.model.layers.2.sublayer.0.norm.a_2', 'Transformer_reduce.model.layers.2.sublayer.0.norm.b_2', 'Transformer_reduce.model.layers.2.sublayer.1.norm.a_2', 'Transformer_reduce.model.layers.2.sublayer.1.norm.b_2', 'Transformer_reduce.model.norm.a_2', 'Transformer_reduce.model.norm.b_2', 'fcn.0.weight', 'fcn.0.bias', 'fcn.0.running_mean', 'fcn.0.running_var', 'fcn.0.num_batches_tracked', 'fcn.1.weight', 'fcn.1.bias', 'fcn_1.0.weight', 'fcn_1.0.bias', 'fcn_1.0.running_mean', 'fcn_1.0.running_var', 'fcn_1.0.num_batches_tracked', 'fcn_1.1.weight', 'fcn_1.1.bias'])\n",
            "読み込むモデル:  checkpoint/pretrained/no_refine_4365.pth\n",
            "pre_dict keys:  odict_keys(['encoder.0.weight', 'encoder.0.bias', 'encoder.1.weight', 'encoder.1.bias', 'encoder.1.running_mean', 'encoder.1.running_var', 'encoder.1.num_batches_tracked', 'Transformer.pos_embedding', 'Transformer.model.layers.0.self_attn.linears.0.weight', 'Transformer.model.layers.0.self_attn.linears.0.bias', 'Transformer.model.layers.0.self_attn.linears.1.weight', 'Transformer.model.layers.0.self_attn.linears.1.bias', 'Transformer.model.layers.0.self_attn.linears.2.weight', 'Transformer.model.layers.0.self_attn.linears.2.bias', 'Transformer.model.layers.0.self_attn.linears.3.weight', 'Transformer.model.layers.0.self_attn.linears.3.bias', 'Transformer.model.layers.0.feed_forward.w_1.weight', 'Transformer.model.layers.0.feed_forward.w_1.bias', 'Transformer.model.layers.0.feed_forward.w_2.weight', 'Transformer.model.layers.0.feed_forward.w_2.bias', 'Transformer.model.layers.0.sublayer.0.norm.a_2', 'Transformer.model.layers.0.sublayer.0.norm.b_2', 'Transformer.model.layers.0.sublayer.1.norm.a_2', 'Transformer.model.layers.0.sublayer.1.norm.b_2', 'Transformer.model.layers.1.self_attn.linears.0.weight', 'Transformer.model.layers.1.self_attn.linears.0.bias', 'Transformer.model.layers.1.self_attn.linears.1.weight', 'Transformer.model.layers.1.self_attn.linears.1.bias', 'Transformer.model.layers.1.self_attn.linears.2.weight', 'Transformer.model.layers.1.self_attn.linears.2.bias', 'Transformer.model.layers.1.self_attn.linears.3.weight', 'Transformer.model.layers.1.self_attn.linears.3.bias', 'Transformer.model.layers.1.feed_forward.w_1.weight', 'Transformer.model.layers.1.feed_forward.w_1.bias', 'Transformer.model.layers.1.feed_forward.w_2.weight', 'Transformer.model.layers.1.feed_forward.w_2.bias', 'Transformer.model.layers.1.sublayer.0.norm.a_2', 'Transformer.model.layers.1.sublayer.0.norm.b_2', 'Transformer.model.layers.1.sublayer.1.norm.a_2', 'Transformer.model.layers.1.sublayer.1.norm.b_2', 'Transformer.model.layers.2.self_attn.linears.0.weight', 'Transformer.model.layers.2.self_attn.linears.0.bias', 'Transformer.model.layers.2.self_attn.linears.1.weight', 'Transformer.model.layers.2.self_attn.linears.1.bias', 'Transformer.model.layers.2.self_attn.linears.2.weight', 'Transformer.model.layers.2.self_attn.linears.2.bias', 'Transformer.model.layers.2.self_attn.linears.3.weight', 'Transformer.model.layers.2.self_attn.linears.3.bias', 'Transformer.model.layers.2.feed_forward.w_1.weight', 'Transformer.model.layers.2.feed_forward.w_1.bias', 'Transformer.model.layers.2.feed_forward.w_2.weight', 'Transformer.model.layers.2.feed_forward.w_2.bias', 'Transformer.model.layers.2.sublayer.0.norm.a_2', 'Transformer.model.layers.2.sublayer.0.norm.b_2', 'Transformer.model.layers.2.sublayer.1.norm.a_2', 'Transformer.model.layers.2.sublayer.1.norm.b_2', 'Transformer.model.norm.a_2', 'Transformer.model.norm.b_2', 'Transformer_reduce.model.pos_embedding_1', 'Transformer_reduce.model.pos_embedding_2', 'Transformer_reduce.model.pos_embedding_3', 'Transformer_reduce.model.layers.0.self_attn.linears.0.weight', 'Transformer_reduce.model.layers.0.self_attn.linears.0.bias', 'Transformer_reduce.model.layers.0.self_attn.linears.1.weight', 'Transformer_reduce.model.layers.0.self_attn.linears.1.bias', 'Transformer_reduce.model.layers.0.self_attn.linears.2.weight', 'Transformer_reduce.model.layers.0.self_attn.linears.2.bias', 'Transformer_reduce.model.layers.0.self_attn.linears.3.weight', 'Transformer_reduce.model.layers.0.self_attn.linears.3.bias', 'Transformer_reduce.model.layers.0.feed_forward.w_1.weight', 'Transformer_reduce.model.layers.0.feed_forward.w_1.bias', 'Transformer_reduce.model.layers.0.feed_forward.w_2.weight', 'Transformer_reduce.model.layers.0.feed_forward.w_2.bias', 'Transformer_reduce.model.layers.0.sublayer.0.norm.a_2', 'Transformer_reduce.model.layers.0.sublayer.0.norm.b_2', 'Transformer_reduce.model.layers.0.sublayer.1.norm.a_2', 'Transformer_reduce.model.layers.0.sublayer.1.norm.b_2', 'Transformer_reduce.model.layers.1.self_attn.linears.0.weight', 'Transformer_reduce.model.layers.1.self_attn.linears.0.bias', 'Transformer_reduce.model.layers.1.self_attn.linears.1.weight', 'Transformer_reduce.model.layers.1.self_attn.linears.1.bias', 'Transformer_reduce.model.layers.1.self_attn.linears.2.weight', 'Transformer_reduce.model.layers.1.self_attn.linears.2.bias', 'Transformer_reduce.model.layers.1.self_attn.linears.3.weight', 'Transformer_reduce.model.layers.1.self_attn.linears.3.bias', 'Transformer_reduce.model.layers.1.feed_forward.w_1.weight', 'Transformer_reduce.model.layers.1.feed_forward.w_1.bias', 'Transformer_reduce.model.layers.1.feed_forward.w_2.weight', 'Transformer_reduce.model.layers.1.feed_forward.w_2.bias', 'Transformer_reduce.model.layers.1.sublayer.0.norm.a_2', 'Transformer_reduce.model.layers.1.sublayer.0.norm.b_2', 'Transformer_reduce.model.layers.1.sublayer.1.norm.a_2', 'Transformer_reduce.model.layers.1.sublayer.1.norm.b_2', 'Transformer_reduce.model.layers.2.self_attn.linears.0.weight', 'Transformer_reduce.model.layers.2.self_attn.linears.0.bias', 'Transformer_reduce.model.layers.2.self_attn.linears.1.weight', 'Transformer_reduce.model.layers.2.self_attn.linears.1.bias', 'Transformer_reduce.model.layers.2.self_attn.linears.2.weight', 'Transformer_reduce.model.layers.2.self_attn.linears.2.bias', 'Transformer_reduce.model.layers.2.self_attn.linears.3.weight', 'Transformer_reduce.model.layers.2.self_attn.linears.3.bias', 'Transformer_reduce.model.layers.2.feed_forward.w_1.weight', 'Transformer_reduce.model.layers.2.feed_forward.w_1.bias', 'Transformer_reduce.model.layers.2.feed_forward.w_2.weight', 'Transformer_reduce.model.layers.2.feed_forward.w_2.bias', 'Transformer_reduce.model.layers.2.sublayer.0.norm.a_2', 'Transformer_reduce.model.layers.2.sublayer.0.norm.b_2', 'Transformer_reduce.model.layers.2.sublayer.1.norm.a_2', 'Transformer_reduce.model.layers.2.sublayer.1.norm.b_2', 'Transformer_reduce.model.norm.a_2', 'Transformer_reduce.model.norm.b_2', 'fcn.0.weight', 'fcn.0.bias', 'fcn.0.running_mean', 'fcn.0.running_var', 'fcn.0.num_batches_tracked', 'fcn.1.weight', 'fcn.1.bias', 'fcn_1.0.weight', 'fcn_1.0.bias', 'fcn_1.0.running_mean', 'fcn_1.0.running_var', 'fcn_1.0.num_batches_tracked', 'fcn_1.1.weight', 'fcn_1.1.bias'])\n",
            "\n",
            "Generating 3D pose...\n",
            "  0% 20/4909 [00:09<39:54,  2.04it/s]/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py:530: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  warnings.warn(\n",
            "100% 4909/4909 [40:54<00:00,  2.00it/s]\n",
            "Generating 3D pose successful!\n",
            "\n",
            "Generating demo...\n",
            "  8% 396/4909 [04:23<51:58,  1.45it/s]^C\n"
          ]
        }
      ],
      "source": [
        "!python demo/vis.py --video \"2018WCB_60_P2_SELLO Edwin_BOT_NAZIR BIN Abdou_MAD.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VuhFJtiC2Q_",
        "outputId": "623439fb-3ca4-425c-9555-9bdab9e3caa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N9ClR6sDF9o",
        "outputId": "1ecf67a1-7275-4cce-9719-81e32b447b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '/content/StridedTransformer-Pose3D/demo/output/2018WCB_60_P2_SELLO Edwin_BOT_NAZIR BIN Abdou_MAD/pose2D/%04d_2D.png':\n",
            "  Duration: 00:03:16.36, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 1280x720, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/content/drive/MyDrive/Colab Notebooks/sports_compe/original_stp3d.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1280x720, q=-1--1, 25 fps, 12800 tbn, 25 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame= 4909 fps= 25 q=-1.0 Lsize=   69214kB time=00:03:16.24 bitrate=2889.3kbits/s speed=1.01x    \n",
            "video:69155kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.084734%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mframe I:20    Avg QP:20.50  size:105786\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mframe P:1310  Avg QP:23.14  size: 27529\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mframe B:3579  Avg QP:27.57  size:  9119\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mconsecutive B-frames:  1.6%  2.0%  4.5% 91.8%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mmb I  I16..4: 12.9% 45.7% 41.4%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mmb P  I16..4:  5.1%  7.4%  4.7%  P16..4: 36.5% 17.2% 10.6%  0.0%  0.0%    skip:18.5%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mmb B  I16..4:  0.9%  1.1%  1.3%  B16..8: 40.3%  6.3%  1.5%  direct: 1.6%  skip:47.0%  L0:44.8% L1:49.9% BI: 5.3%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0m8x8 transform intra:40.3% inter:59.1%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mcoded y,uvDC,uvAC intra: 50.5% 82.7% 48.7% inter: 11.6% 18.5% 3.9%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mi16 v,h,dc,p: 18% 57% 12% 13%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 16% 27% 26%  4%  5%  5%  6%  5%  6%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 22% 24% 18%  5%  6%  7%  7%  6%  5%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mi8c dc,h,v,p: 36% 39% 16%  9%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mWeighted P-Frames: Y:0.2% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mref P L0: 67.3% 11.8% 15.0%  6.0%  0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mref B L0: 88.5%  9.3%  2.2%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mref B L1: 95.8%  4.2%\n",
            "\u001b[1;36m[libx264 @ 0x55ac0c24de00] \u001b[0mkb/s:2885.07\n"
          ]
        }
      ],
      "source": [
        "#  2次元の貼り付きを確認\n",
        "frames_path = \"/content/StridedTransformer-Pose3D/demo/output/2018WCB_60_P2_SELLO Edwin_BOT_NAZIR BIN Abdou_MAD/pose2D/%04d_2D.png\"\n",
        "result_video = \"/content/drive/MyDrive/Colab Notebooks/sports_compe/original_stp3d.mp4\"\n",
        "\n",
        "!ffmpeg -i \"/content/StridedTransformer-Pose3D/demo/output/2018WCB_60_P2_SELLO Edwin_BOT_NAZIR BIN Abdou_MAD/pose2D/%04d_2D.png\" -c:v libx264 -vf \"fps=25,format=yuv420p\" \"/content/drive/MyDrive/Colab Notebooks/sports_compe/original_stp3d.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JA1capVfzUdw",
        "outputId": "a623d02f-b29e-4852-9c53-50ee7259c32f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '/content/StridedTransformer-Pose3D/demo/output/blueplayer_clipping_movie/pose3D/%04d_3D.png':\n",
            "  Duration: 00:02:40.36, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgba(pc), 878x878 [SAR 7874:7874 DAR 1:1], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "\u001b[1;35m[NULL @ 0x564cdba22c00] \u001b[0m\u001b[1;31mUnable to find a suitable output format for '/content/drive/MyDrive/Colab'\n",
            "\u001b[0m\u001b[1;31m/content/drive/MyDrive/Colab: Invalid argument\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#  3次元の貼り付きを確認\n",
        "frames_path = \"/content/StridedTransformer-Pose3D/demo/output/blueplayer_clipping_movie/pose3D/%04d_3D.png\"\n",
        "result_video = \"/content/drive/MyDrive/Colab Notebooks/sports_compe/blueplayer_clipping_3Dposemovie.mp4\"\n",
        "\n",
        "!ffmpeg -i {frames_path} -c:v libx264 -vf \"fps=25,format=yuv420p\" {result_video}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wedm57gMlV96"
      },
      "source": [
        "# ↓使わない"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_pLVu0MlT9T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f07BZGVqlUA2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkJB9ynslUEI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqQFk0IwEqUV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0MFu5oFEqW9"
      },
      "outputs": [],
      "source": [
        "!cp /content/StridedTransformer-Pose3D/demo/output/clipping_video/clipping_video.mp4 /content/drive/MyDrive/D2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kcx3OnBIM8X"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/StridedTransformer-Pose3D/demo/output/clipping_video '/content/drive/MyDrive/D2/スポコン2022'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkpP9Z9_INBS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmfbZFQe1UD0"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "i = 6\n",
        "\n",
        "with open(f'/content/drive/MyDrive/D2/POSE/kobudo_test{i}.csv', 'w') as f:\n",
        "    writer = csv.writer(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdXEcGo6QLgs"
      },
      "outputs": [],
      "source": [
        "ax.plot(x, y, z, lw=2, color=(0, 0, 0))\n",
        "ax.scatter(x, y, z, c='green')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3MqDKt0mWk0"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/StridedTransformer-Pose3D/demo/output/tsuki_nakajima_sense/input_2D '/content/drive/MyDrive/D2/POSE/kobudo_videos/220905/output/tsuki_nakajima_sense'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvP7atetepeW"
      },
      "outputs": [],
      "source": [
        "npy_path = '/content/drive/MyDrive/D2/POSE/kobudo_videos/220906_tsuki_yogi_sense/3d_150.npy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3md4I1z1epha",
        "outputId": "5bdee0b3-adc5-4048-ab9b-92179f78c061"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.12919983],\n",
              "       [ 0.        , -0.06041094],\n",
              "       [ 0.38336796,  0.3933114 ]], dtype=float32)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "coord_data = np.load(npy_path)\n",
        "coord_data[0]\n",
        "#coord_data[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cqvDPNorQnF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCsr-HKWGH0"
      },
      "outputs": [],
      "source": [
        "person = 'nakajima'\n",
        "conduct = 'tsuki'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4_Aq4xwRuxu",
        "outputId": "ff24c897-774d-4e09-d41e-938db7483656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/D2/POSE/kobudo_videos/220906_tsuki_nakajima_sense/3d_0.npy\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from natsort import natsorted\n",
        "\n",
        "all_coord_dir = f'/content/drive/MyDrive/D2/POSE/kobudo_videos/220906_{conduct}_{person}_sense'\n",
        "all_coord_names = os.listdir(all_coord_dir)\n",
        "\n",
        "all_coord_path = os.path.join(all_coord_dir, all_coord_names[0])\n",
        "print(all_coord_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fn3EQCE3VX7e",
        "outputId": "1403ebcb-4547-41b7-c07e-d7006c73e3b8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/x1_blue_reconstract_tsuki_nakajima_sense.mp4'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video_path = f'/content/drive/MyDrive/D2/POSE/kobudo_videos/220905/cropped/{conduct}_{person}_sense.mp4'\n",
        "\n",
        "video_name = os.path.basename(video_path)\n",
        "\n",
        "dest_path = f'/content/x1_blue_reconstract_{video_name}'\n",
        "dest_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKmC9Ig8kgTm"
      },
      "outputs": [],
      "source": [
        "bodypart_list = [\n",
        "    'RHip', 'LHip', 'RKnee', 'LKnee', 'RAnkle', 'LAnkle', 'Spine', 'Neck',\n",
        "    'RShoulder', 'LShoulder', 'RElbow', 'RWrist', 'LElbow', 'LWrist', 'Site', 'Head'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhOG2i3ix2zf",
        "outputId": "ab6c7c44-e3cb-4efc-9b77-e9777b97fb5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['RHip_x', 'RHip_y', 'RHip_z', 'LHip_x', 'LHip_y', 'LHip_z', 'RKnee_x', 'RKnee_y', 'RKnee_z', 'LKnee_x', 'LKnee_y', 'LKnee_z', 'RAnkle_x', 'RAnkle_y', 'RAnkle_z', 'LAnkle_x', 'LAnkle_y', 'LAnkle_z', 'Spine_x', 'Spine_y', 'Spine_z', 'Neck_x', 'Neck_y', 'Neck_z', 'RShoulder_x', 'RShoulder_y', 'RShoulder_z', 'LShoulder_x', 'LShoulder_y', 'LShoulder_z', 'RElbow_x', 'RElbow_y', 'RElbow_z', 'RWrist_x', 'RWrist_y', 'RWrist_z', 'LElbow_x', 'LElbow_y', 'LElbow_z', 'LWrist_x', 'LWrist_y', 'LWrist_z', 'Site_x', 'Site_y', 'Site_z', 'Head_x', 'Head_y', 'Head_z']\n"
          ]
        }
      ],
      "source": [
        "from itertools import chain\n",
        "\n",
        "columns_name_list = [[n+'_x', n+'_y', n+'_z'] for n in bodypart_list]\n",
        "column_names = list(chain(*columns_name_list))\n",
        "print(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKKnr3QPm-Wy"
      },
      "outputs": [],
      "source": [
        "video_output_dir = '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "912HWDk2UXz2",
        "outputId": "5b0b70f8-4ad0-4261-8055-3dec31b7d5b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tsuki_nakajima_sense.mp4 の csvファイルが存在しないため, 作成します.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# dataset 用のcsv(ヘッダー)を用意\n",
        "dataset_root = os.path.join(video_output_dir, 'dataset')\n",
        "os.makedirs(dataset_root, exist_ok=True)\n",
        "\n",
        "# csv保存先\n",
        "csv_name = video_name.rsplit('.', 1)[0]\n",
        "csv_output = os.path.join(dataset_root, f'{csv_name}_3d_data.csv')\n",
        "\n",
        "# 記録したい内容\n",
        "dataset_list = column_names # 各関節名(x, y, z): 16 * 3\n",
        "#dataset_list.extend(['test', 'test2'])\n",
        "\n",
        "if not os.path.exists(csv_output):\n",
        "    os.makedirs(dataset_root, exist_ok=True)\n",
        "    print(f\"{video_name} の csvファイルが存在しないため, 作成します.\")\n",
        "\n",
        "    dataset_columns = dataset_list\n",
        "\n",
        "    dataset_template = pd.DataFrame(columns=dataset_columns)\n",
        "    dataset_template.to_csv(csv_output, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7wznjVmkO1H",
        "outputId": "eab49014-c280-4fae-90e4-46a9387f9324"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "connections[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpUJ1RFERxXy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "connections = [[0, 1], [0, 2], [2, 4], [1, 3], [3, 5],\n",
        "                   [6, 7], [7, 8], [8, 10], [10, 11], [7, 9], [9, 12],\n",
        "                    [12, 13], [7, 14], [14, 15], [15, 16], [16, 17]]\n",
        "\n",
        "def show3Dpose(ax, coord_data):\n",
        "    ax.cla()\n",
        "    ax.view_init(elev=15., azim=70)\n",
        "\n",
        "    coord_list = []\n",
        "    draw_list = []\n",
        "    for e, c in enumerate(coord_data):\n",
        "        x, y, z = c[0], c[1], c[2]\n",
        "\n",
        "        #if e % 2 == 0:\n",
        "        #ax.plot(x, y, z, lw=2)\n",
        "        #ax.scatter(x, y, z)\n",
        "        #ax.scatter(x[0], y[0], z[0], color='red')\n",
        "        ax.scatter(x[1], y[1], z[1], color='blue')\n",
        "        coord_list.append(x[1])\n",
        "        coord_list.append(y[1])\n",
        "        coord_list.append(z[1])\n",
        "        draw_list.append([x, y, z])\n",
        "        #ax.text(x[1], y[1], z[1], f\"{e}\", color='r')\n",
        "\n",
        "    # 接続線の描画\n",
        "    hip_coord = []\n",
        "    for i in range(len(connections)):\n",
        "        c = draw_list[i]\n",
        "        x, y, z = c[0], c[1], c[2]\n",
        "\n",
        "        ax.plot(x, y, z, lw=2, color='k')\n",
        "\n",
        "        if i == 6: # 腰の中心を描画\n",
        "            hip_coord.append([x[0], y[0], z[0]])\n",
        "    hx, hy, hz = hip_coord[0][0], hip_coord[0][1], hip_coord[0][2]\n",
        "    ax.scatter(hx, hy, hz, color='red')\n",
        "\n",
        "    with open(csv_output, 'a') as csv_f:\n",
        "        c_writer = csv.writer(csv_f)\n",
        "        c_writer.writerow(coord_list)\n",
        "\n",
        "    RADIUS = 0.8\n",
        "\n",
        "    ax.set_xlim3d([-RADIUS, RADIUS])\n",
        "    ax.set_ylim3d([-RADIUS, RADIUS])\n",
        "    ax.set_aspect('equal') # works fine in matplotlib==2.2.2\n",
        "\n",
        "    # 軸ラベルを設定\n",
        "    ax.set_xlabel(\"x\", size = 14)\n",
        "    ax.set_ylabel(\"y\", size = 14)\n",
        "    ax.set_zlabel(\"z\", size = 14)\n",
        "\n",
        "    #white = (1.0, 1.0, 1.0, 0.0)\n",
        "    # ax.xaxis.set_pane_color(white)\n",
        "    # ax.yaxis.set_pane_color(white)\n",
        "    # ax.zaxis.set_pane_color(white)\n",
        "\n",
        "    ax.tick_params('x', labelbottom = False)\n",
        "    ax.tick_params('y', labelleft = False)\n",
        "    ax.tick_params('z', labelleft = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjv9ZY1-myI7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "# Use Agg backend for canvas\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "\n",
        "# Create a figure of the right size with one axes that takes up the full figure\n",
        "dpi = 100\n",
        "figsize = w / float(dpi), h / float(dpi)\n",
        "\n",
        "fig = plt.figure(figsize=figsize)\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "ax.axis('off')\n",
        "\n",
        "gs = gridspec.GridSpec(1, 1)\n",
        "gs.update(wspace=-0.00, hspace=0.05)\n",
        "ax = plt.subplot(gs[0], projection='3d')\n",
        "ax.view_init(elev=15., azim=120)\n",
        "\n",
        "canvas = FigureCanvas(fig)\n",
        "canvas.draw()\n",
        "mat = np.array(canvas.renderer._renderer)\n",
        "mat = cv2.cvtColor(mat, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "writer = cv2.VideoWriter(dest_path, fourcc, fps, (mat.shape[1], mat.shape[0]))\n",
        "\n",
        "for e, file_name in enumerate(all_coord_names):\n",
        "    npy_path = os.path.join(all_coord_dir, file_name)\n",
        "    coord_data = np.load(npy_path)\n",
        "    show3Dpose(ax, coord_data)\n",
        "\n",
        "    canvas = FigureCanvas(fig)\n",
        "    canvas.draw()\n",
        "    mat = np.array(canvas.renderer._renderer)\n",
        "    mat = cv2.cvtColor(mat, cv2.COLOR_RGB2BGR)\n",
        "    cv2.putText(mat, f'{e}', (100, 100), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 0), 2, cv2.LINE_AA)\n",
        "    #plt.imshow(mat)\n",
        "\n",
        "    writer.write(mat)\n",
        "\n",
        "cap.release()\n",
        "writer.release()\n",
        "cv2.destroyAllWindows()\n",
        "#plt.savefig('/content/test.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLvGKKv719XJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4gwxDDf0bMj"
      },
      "outputs": [],
      "source": [
        "!pip install numpy vg\n",
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4ZZXha4BGMa"
      },
      "outputs": [],
      "source": [
        "def get_angle(a, b, c):\n",
        "  # ベクトルを定義\n",
        "  vec_a = a - b\n",
        "  vec_c = c - b\n",
        "\n",
        "  # コサインの計算\n",
        "  length_vec_a = np.linalg.norm(vec_a)\n",
        "  length_vec_c = np.linalg.norm(vec_c)\n",
        "  inner_product = np.inner(vec_a, vec_c)\n",
        "  cos = inner_product / (length_vec_a * length_vec_c)\n",
        "\n",
        "  # 角度（ラジアン）の計算\n",
        "  rad = np.arccos(cos)\n",
        "\n",
        "  # 弧度法から度数法（rad ➔ 度）への変換\n",
        "  degree = np.rad2deg(rad)\n",
        "  return degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XfJF0FA0SHj",
        "outputId": "a2db07e6-7595-4774-9ce7-638911cac7b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(coord_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPFpZ55h-WfH"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcqCQ6hV15Zj"
      },
      "outputs": [],
      "source": [
        "right_ankle = coord_data[4] # 右足首\n",
        "left_ankle = coord_data[5] # 左足首\n",
        "\n",
        "right_knee = coord_data[2] # 右膝\n",
        "left_knee = coord_data[3] # 左膝\n",
        "\n",
        "right_hip = coord_data[0] # 右腰\n",
        "left_hip = coord_data[1] # 左腰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uskohVR41izL",
        "outputId": "3588d541-d2cb-4cd5-d362-f070cbb7dfce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6244918 0.46505463\n",
            "0.59836876 0.4690552\n"
          ]
        }
      ],
      "source": [
        "r_lower_vec = np.linalg.norm(right_knee - right_ankle)\n",
        "r_upper_vec = np.linalg.norm(right_hip - right_knee)\n",
        "print(r_lower_vec, r_upper_vec)\n",
        "\n",
        "l_lower_vec = np.linalg.norm(left_knee - left_ankle)\n",
        "l_upper_vec = np.linalg.norm(left_hip - left_knee)\n",
        "print(l_lower_vec, l_upper_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDRlOBZg-oRQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, 420)\n",
        "ret, image = cap.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5f0CV-z-eAZ",
        "outputId": "e381df4e-1dfb-4c97-92e1-ef3f11946106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "164.375986585422\n"
          ]
        }
      ],
      "source": [
        "with mp_pose.Pose(\n",
        "    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:\n",
        "    # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
        "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # Print nose landmark.\n",
        "    image_hight, image_width, _ = image.shape\n",
        "\n",
        "    lx_hip = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].x * image_width\n",
        "    ly_hip = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].y * image_hight\n",
        "    hip = (lx_hip, ly_hip)\n",
        "    hip = np.array([hip[0], hip[1]])\n",
        "\n",
        "    lx_knee = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE].x * image_width\n",
        "    ly_knee = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE].y * image_hight\n",
        "    knee = (lx_knee, ly_knee)\n",
        "    knee = np.array([knee[0], knee[1]])\n",
        "\n",
        "    lx_ankle = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ANKLE].x * image_width\n",
        "    ly_ankle = results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ANKLE].y * image_hight\n",
        "    ankle = (lx_ankle, ly_ankle)\n",
        "    ankle = np.array([ankle[0], ankle[1]])\n",
        "\n",
        "    #print(l_ankle, r_ankle)\n",
        "\n",
        "    t_angle = get_angle(ankle, knee, hip)\n",
        "    print(t_angle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpI3OxEjDEHN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 164.375986585422"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDIvw6Yk0SLc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import vg\n",
        "\n",
        "r_vec1 = np.array([right_knee])\n",
        "r_vec2 = np.array([right_ankle])\n",
        "\n",
        "vg.angle(r_vec1, r_vec2, look=vg.basis.x, units='deg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o8XLzzq2aTe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJWGwlZP2acE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fIjgFJw2bb8"
      },
      "source": [
        "（提供する）csvデータから再構成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ux6iXWuH3laI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "def show3Dpose(ax, x_list, y_list, z_list):\n",
        "    ax.cla()\n",
        "    ax.view_init(elev=15., azim=70)\n",
        "\n",
        "    x_list = [np.array(x, dtype=np.float32) for x in x_list]\n",
        "    y_list = [np.array(y, dtype=np.float32) for y in y_list]\n",
        "    z_list = [np.array(z, dtype=np.float32) for z in z_list]\n",
        "\n",
        "    ax.scatter(x_list, y_list, z_list, color='blue')\n",
        "\n",
        "    RADIUS = 0.8\n",
        "\n",
        "    ax.set_xlim3d([-RADIUS, RADIUS])\n",
        "    ax.set_ylim3d([-RADIUS, RADIUS])\n",
        "    ax.set_aspect('equal') # works fine in matplotlib==2.2.2\n",
        "\n",
        "    white = (1.0, 1.0, 1.0, 0.0)\n",
        "    ax.xaxis.set_pane_color(white)\n",
        "    ax.yaxis.set_pane_color(white)\n",
        "    ax.zaxis.set_pane_color(white)\n",
        "\n",
        "    ax.tick_params('x', labelbottom = False)\n",
        "    ax.tick_params('y', labelleft = False)\n",
        "    ax.tick_params('z', labelleft = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNl6-S1a4DW3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "# Use Agg backend for canvas\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "\n",
        "# Create a figure of the right size with one axes that takes up the full figure\n",
        "dpi = 100\n",
        "figsize = w / float(dpi), h / float(dpi)\n",
        "\n",
        "fig = plt.figure(figsize=figsize)\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "ax.axis('off')\n",
        "\n",
        "gs = gridspec.GridSpec(1, 1)\n",
        "gs.update(wspace=-0.00, hspace=0.05)\n",
        "ax = plt.subplot(gs[0], projection='3d')\n",
        "ax.view_init(elev=15., azim=120)\n",
        "\n",
        "canvas = FigureCanvas(fig)\n",
        "canvas.draw()\n",
        "mat = np.array(canvas.renderer._renderer)\n",
        "mat = cv2.cvtColor(mat, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "writer = cv2.VideoWriter('/content/test_rec_show.mp4', fourcc, fps, (mat.shape[1], mat.shape[0]))\n",
        "\n",
        "with open(csv_output) as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        x_list = row[0::3]\n",
        "        y_list = row[1::3]\n",
        "        z_list = row[2::3]\n",
        "\n",
        "        show3Dpose(ax, x_list, y_list, z_list)\n",
        "\n",
        "        canvas = FigureCanvas(fig)\n",
        "        canvas.draw()\n",
        "        mat = np.array(canvas.renderer._renderer)\n",
        "        mat = cv2.cvtColor(mat, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        writer.write(mat)\n",
        "\n",
        "cap.release()\n",
        "writer.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxbKb-cL5WNw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5vx6P_j5WR8"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/dataset '/content/drive/MyDrive/D2/POSE/kobudo_videos'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaZdsrEW9v5C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gez8qWNT9v7w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC7lNgh82wZF"
      },
      "outputs": [],
      "source": [
        "with open(csv_output) as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        print(row)\n",
        "        x_list = row[0::3]\n",
        "        y_list = row[1::3]\n",
        "        z_list = row[2::3]\n",
        "        print(x_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7cXrfRb2auH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0BjlPv9UX29"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiGppFhmUX6K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVtjYL2oUX8S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def show3Dpose(vals, ax):\n",
        "    ax.cla()\n",
        "    ax.view_init(elev=15., azim=70)\n",
        "\n",
        "    I = np.array( [0, 0, 1, 4, 2, 5, 0, 7,  8,  8, 14, 15, 11, 12, 8,  9])\n",
        "    J = np.array( [1, 4, 2, 5, 3, 6, 7, 8, 14, 11, 15, 16, 12, 13, 9, 10])\n",
        "    LR = np.array([0, 1, 0, 1, 0, 1, 0, 0, 0,   1,  0,  0,  1,  1, 0, 0], dtype=bool)\n",
        "\n",
        "    coord_list = []\n",
        "    for i in np.arange( len(I) ):\n",
        "        x, y, z = [np.array( [vals[I[i], j], vals[J[i], j]] ) for j in range(3)]\n",
        "        #print(\"x, y, z: \", x, y, z)\n",
        "\n",
        "        ax.plot(x, y, z, lw=2)\n",
        "        ax.scatter(x, y, z)\n",
        "        #coord_list.append([x, y, z])\n",
        "        # coord_list.extend([x[0], x[1]])\n",
        "        # coord_list.extend([y[0], y[1]])\n",
        "        # coord_list.extend([z[0], z[1]])\n",
        "\n",
        "    RADIUS = 0.8\n",
        "\n",
        "    ax.set_xlim3d([-RADIUS, RADIUS])\n",
        "    ax.set_ylim3d([-RADIUS, RADIUS])\n",
        "    ax.set_aspect('equal') # works fine in matplotlib==2.2.2\n",
        "\n",
        "    white = (1.0, 1.0, 1.0, 0.0)\n",
        "    ax.xaxis.set_pane_color(white)\n",
        "    ax.yaxis.set_pane_color(white)\n",
        "    ax.zaxis.set_pane_color(white)\n",
        "\n",
        "    ax.tick_params('x', labelbottom = False)\n",
        "    ax.tick_params('y', labelleft = False)\n",
        "    ax.tick_params('z', labelleft = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN-1imwym-ZT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uS4qk33Qki0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([0.        , 0.12568605], dtype=np.float32)\n",
        "y = np.array([ 0.       , -0.0335251], dtype=np.float32)\n",
        "z = np.array([0.5752221, 0.5574139], dtype=np.float32)\n",
        "\n",
        "x2 = np.array([ 0.        , -0.12461963], dtype=np.float32)\n",
        "y2 = np.array([0.        , 0.03290185], dtype=np.float32)\n",
        "z2 = np.array([0.5752221 , 0.59335494], dtype=np.float32)\n",
        "\n",
        "x3 = np.array([0.12568605, 0.18360424], dtype=np.float32)\n",
        "y3 = np.array([-0.0335251 ,  0.28834122], dtype=np.float32)\n",
        "z3 = np.array([0.5574139 , 0.30513826], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbN8gEaZla_j",
        "outputId": "1c023749-3eee-4173-af9e-363db64397e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.12568605"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f65UxONt26g8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/D2/POSE/kobudo_test6.csv'\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "display(df)\n",
        "\n",
        "row_data = df.iloc[0:1]\n",
        "#row_data = np.asarray(row_data, dtype = float)\n",
        "row_data\n",
        "#coord = np.array()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3BnnMbl1mOG"
      },
      "outputs": [],
      "source": [
        "x_1_list = row_data[0::6]\n",
        "x_2_list = row_data[1::6]\n",
        "\n",
        "y_1_list = row_data[2::6]\n",
        "y_2_list = row_data[3::6]\n",
        "\n",
        "z_1_list = row_data[4::6]\n",
        "z_2_list = row_data[5::6]\n",
        "\n",
        "x_list = []\n",
        "for x1, x2 in zip(x_1_list, x_2_list):\n",
        "  try:\n",
        "    x1, x2 = float(x1), float(x2)\n",
        "    print([x1, x2])\n",
        "    x = np.array([x1, x2], dtype=np.float32)\n",
        "    x_list.append(x)\n",
        "  except ValueError as e:\n",
        "    print(\"error\", e)\n",
        "\n",
        "# x_list = [np.array([x1, x2], dtype=np.float32) for x1, x2 in zip(x_1_list, x_2_list)]\n",
        "# y_list = [np.array([y1, y2], dtype=np.float32) for y1, y2 in zip(y_1_list, y_2_list)]\n",
        "# z_list = [np.array([z1, z2], dtype=np.float32) for z1, z2 in zip(z_1_list, z_2_list)]\n",
        "\n",
        "# print(x_list)\n",
        "# print(y_list)\n",
        "# print(z_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qndRjUjffyE"
      },
      "outputs": [],
      "source": [
        "# The 17 joints - in order - are:\n",
        "#{'Pelvis' 'RHip' 'RKnee' 'RAnkle' 'LHip' 'LKnee' 'LAnkle' 'Spine1'\n",
        "#'Neck' 'Head' 'Site' 'LShoulder' 'LElbow' 'LWrist' 'RShoulder' 'RElbow' 'RWrist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlPy2mTE26nA"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "plt.switch_backend('agg')\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "\n",
        "fig = plt.figure( figsize=(9.6, 5.4))\n",
        "gs = gridspec.GridSpec(1, 1)\n",
        "gs.update(wspace=-0.00, hspace=0.05)\n",
        "ax = plt.subplot(gs[0], projection='3d')\n",
        "\n",
        "ax.plot(x, y, z, lw=2)\n",
        "ax.scatter(x, y, z)\n",
        "\n",
        "ax.plot(x2, y2, z2, lw=2)\n",
        "ax.scatter(x2, y2, z2)\n",
        "\n",
        "ax.plot(x3, y3, z3, lw=2)\n",
        "ax.scatter(x3, y3, z3)\n",
        "\n",
        "RADIUS = 0.8\n",
        "\n",
        "ax.set_xlim3d([-RADIUS, RADIUS])\n",
        "ax.set_ylim3d([-RADIUS, RADIUS])\n",
        "ax.set_aspect('equal') # works fine in matplotlib==2.2.2\n",
        "\n",
        "white = (1.0, 1.0, 1.0, 0.0)\n",
        "ax.xaxis.set_pane_color(white)\n",
        "ax.yaxis.set_pane_color(white)\n",
        "ax.zaxis.set_pane_color(white)\n",
        "\n",
        "ax.tick_params('x', labelbottom = False)\n",
        "ax.tick_params('y', labelleft = False)\n",
        "ax.tick_params('z', labelleft = False)\n",
        "\n",
        "plt.show()\n",
        "plt.savefig('/content/test.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfQLpEC6RUxx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}